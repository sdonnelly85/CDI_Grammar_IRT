---
title: Psychometric modeling of the Syntactic Complexity Scale of the CDI
author: Seamus Donnelly
date created: July 5, 2021
date compiled: "`r Sys.Date()`"
output: 
  html_document:
    df_print: paged
    toc: true
    toc_float: true
    number_sections: true
---

```{r message=FALSE, warning=FALSE}
library(wordbankr) # WB data
library(tidyverse) # tidy
library(mirt) # IRT models
library(ltm) # more IRT functions
library(psych) # some psychometric stuff (tests of dimensionality)
library(Gifi)# some more psychometric stuff (tests of dimensionality)
library(knitr) # some formatting, tables, etc
library(patchwork) # combining plots. 
library(GGally) # More plottinng options. 
library(lordif) # differential item functioning.
```


In our meeting on 28/June/2021, we noted that by adding Word Forms, we might be able to model some of the intermediate levels of syntactic proficiency between the 0 and 1 items on the complexity scale. I'm going to start with adding word endings and combines as those seem like the most straight forward. The problem with the word forms and word endings is that we expect them to by non-monotonically related to language proficiency, so I'm no sure how well they'll fit this model. I've also made a new branch on github because I realized I could maintain the ordinal scale of the ordinal items, and use an ordinal IRT model -- since binary items are just ordinal items with 2 categories. This appears to be working alright, but I want to keep it in a second branch until I verify this is appropriate (I'm 95% sure it's right). 


Download data from Word Bank:
```{r get data}
Inst <- get_instrument_data(language="English (American)", form="WS")
Admin <- get_administration_data(language="English (American)", form="WS")
Item <- get_item_data(language="English (American)", form = "WS")
```


The American English CDI data has 7 types of items other than words, which are also listed in the code above: *combine*, *complexity*, *word_endings*, *word_forms_nouns*, *word_forms_verbs*, *word_endings_nouns*, and *word_endings_verbs*. The *word_endings* type seems to be questions about how often a child applies a particular morphological rule (e.g., adding an s to possessives). The *word_forms_verbs* and *word_forms_nouns* seem to be about whether a child can produce specific irregular past tenses or plurals. The *word_endings_nouns* and *word_endings_verbs* seem to be about overregularizations on nouns and verbs.  

Combine data sets. Create binary variable for accuracy.  
```{r format data}
Complex <- Admin %>%
  full_join(.,Inst, by="data_id") %>%
  full_join(., Item, by="num_item_id") %>%
  filter(longitudinal==FALSE) %>%
  filter(type == "combine" | 
           type == "complexity" |
           type == "word_endings" |
           type== "word_forms_nouns" |
           type == "word_forms_verbs" | 
           type == "word_endings_nouns" |
           type == "word_endings_verbs"
         ) %>%
  mutate(
    out = ifelse(value=="complex" | value=="sometimes" | value=="produces", yes=1, 
                 no = ifelse(value=="often", yes=2, no =0))
  ) 
```


As a quick sanity check, let's see which items have ordinal data (to make sure the above code worked correctly). 
```{r}
Complex %>%
  filter(value=="often") %>%
  group_by(definition) %>%
  count()
```
Ok, it's only the combine variable and word endings variables -- that's right. 


## Data Screening
Let's see how many missing values there are. 
```{r count NAs}
Complex %>%
  filter(is.na(out)) %>%
  group_by(definition) %>%
  count() 
```

Looks like there are 1426 participants don't have item-level complexity scores. There are a further 2 who don't have word forms, and 1 who doesn't have word endings. 


```{r}
Complex$complexity_category <- ifelse(Complex$complexity_category == "", yes=Complex$type, no=Complex$complexity_category)
```

Let's look at means and SDs for each item:
```{r Summary Tables}
Complex %>% 
  filter(!is.na(out)) %>%
  group_by(definition) %>%
  summarise(
    mean=mean(out), 
    sd=sd(out),
    sum=sum(out),
    category = first(complexity_category)
  ) %>%
  arrange(category) %>%
  kable(caption="Means and SDs for Each Item (Arranged by Item)")
```


Let's also check the distribution of means for each category
```{r}
Complex %>%
  filter(!is.na(out)) %>%
  group_by(definition) %>%
  summarise(
    mean=mean(out), 
    category = first(complexity_category),
    cat2 = str_replace(category, "_", " " ) %>% str_replace("_", "\n" ) 
  ) %>%
  ggplot(aes(x=cat2, y= mean)) + geom_boxplot() 
```

It seems like the word endings and the combine words variable have higher means than the morphological and syntactic complexity items (though they are ordinal, but when I treated these as binary in an older version the results were the same). The over-regularization items have very low means, and the word forms looks a bit like the syntactic and morphological complexity. 


One difficulty I just thought of is that the we would expect the word form and over-regularizations to be non-monotonially related to overall grammatical proficiency, given the inverse u shaped curve we typically see in the development of morphology. 

Therefore, I'm going to first add the combine item and the word endings variables, see how that changes the results of the IRT model, and then plot the over-regularization and word form scores against this, to see if we see the non-monotonicity. 

## Data Preparation
Prepare data set for IRT modeling. I'm going to re-name each item to its category (morphology or syntax) and its item number so as to make some graphs easier to read. 
```{r}
Complex_short_with_ids <- Complex %>% 
  dplyr::select(data_id, value, out, complexity_category, num_item_id) %>%
  mutate(
    label = str_c(complexity_category, num_item_id)
  ) %>%
  pivot_wider(id_cols=data_id, names_from = "label", values_from="out") %>%
  drop_na()


Complex_short <- Complex_short_with_ids %>%
  dplyr::select(starts_with(c("combine", "morphology", "syntax")), c("word_endings686", "word_endings687", "word_endings688", "word_endings689")) # dataset for IRT can't have IDs
```

## Dimensionality Assessment
Prior to fitting the IRT model, it's worth looking at the correlation structure of the data to get a sense of the possible dimensionality. 

Create tetrachoric correlation matrix (for binary data). I created a vector with labels, for labeling of the correlation plot. 
```{r}
Complex_poly <- polychoric(Complex_short)

rho <- Complex_poly$rho

lab = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", "14", "15", "16", "17", "18", "19", "20", "21", "22", "23", "24", "25", "26", "27", "28", "29", "30", "31", "32", "33", "34", "35", "36", "37", "38") # create labels for corPlot because the defintions are long and take up a ton of space
```
Check out the correlations
```{r}
corPlot(rho, labels=lab)
```

They look really high. Let's examine the dimensionality, using princals, which is basically just a version of PCA that is appropriate for categorical data. 
```{r}
pc <- princals(rho)

plot(pc)
```

Seems like there are more intermediate points in between the two clouds of vectors I saw with the earlier version of the data set. However, the plot has fipped, so that most of the vectors are pointing toward the positive first component. 


```{r , message=FALSE, error=FALSE}
fa.parallel(rho, fa="fa", cor="poly", n.obs = 2785)
```
Parallel analysis suggests one very large factor and a small second one. Although with these additional items, I'm seeing some convergence complaints that I wasn't seeing before. These dimensionality tests aren't of substantive interest, so I'm not especially worried, but it's worth keeping this in mind for the later models. 

A third metric is very simple structure. This tells us the best solution assuming different values of k, where k is the maximum number of latent variables an observed variable loads on. 

```{r, message=FALSE, error=FALSE}
vss(rho, cor="poly", n.obs = 2785)
```

Very simple structure also suggests one, if we assume each variable loads on one factor only. If we assume everything loads on 2 factors then a 2 factor solution is best. Again, note that the algorithm is having difficulty with estimation.   

# IRT Modeling

## 2PL model
Let's first fit the 2pl model and examine the output. 
```{r  message=FALSE, warning=FALSE}
m1 <- mirt(Complex_short, 1, itemtype="graded", verbose=FALSE) # graded because we are treating all variables as ordinal. 
```

```{r}
m1
```




Let's check the overall fit of the model. The chi square statistic here isn't super meaningful given the sample size.  
```{r}
M2(m1)
```

RMSEA and CFI are good, SRMSR is a bit high but pretty reasonable. We can see if individual items misfit -- i.e., do the observed cell counts differ from the expected cell counts. 

```{r}
itemfit(m1)
```

A few items significantly misfit. Let's take everything with a p value less than .10. We'll look at item gam plots of those, which plot the item response probability against the latent variable using a gam function, to look for deviations in the item-response function.   

```{r, message=FALSE, warning=FALSE}
misfit <- itemfit(m1) %>% # Get labels of mis-fitting items. 
  filter(p.S_X2 <= .10) %>%
  dplyr::select(item) %>%
  as.vector()

items_good <- dplyr::select(Complex_short, -all_of(misfit$item)) # Well fitting items
items_bad <- dplyr::select(Complex_short, all_of(misfit$item)) # Poorly fitting items

mod_fit <- mirt(items_good, 1, "graded", verbose=FALSE) # Calculate factor scores using only the well fitting items. 
Theta <- fscores(mod_fit)


```
# Plot non-linear item response theory curves with the factor scores 

```{r}
IG762<- itemGAM(items_bad$morphology762, Theta)
IG764<- itemGAM(items_bad$morphology764, Theta)
IG771<- itemGAM(items_bad$morphology771, Theta)
IG783<- itemGAM(items_bad$syntax783, Theta)
IG792<- itemGAM(items_bad$syntax792, Theta)

```

```{r}



```





Next let's look at the relationship between the true scores and the raw scores. 
```{r}
true_raw <- Complex_short %>%
  mutate(
    Raw = rowSums(.[,1:42]), 
    Theta = fscores(m1, method="MAP")
  ) %>%
  ggplot(aes(x=Theta, y=Raw)) + geom_point() + stat_smooth(method="loess") + theme_minimal()

raw_score <- Complex_short %>%
  mutate(
    Raw = rowSums(.[,1:42])
    ) %>% 
   ggplot(aes(x=Raw)) + geom_histogram() + theme_minimal()

true_score <- Complex_short %>%
  mutate(
    Theta = fscores(m1, method="MAP")
    ) %>% 
   ggplot(aes(x=Theta)) + geom_histogram() + theme_minimal()

library(patchwork)

true_raw/(true_score + raw_score) 
```
Well, we're getting a little more sensitivity at the lower end of the scale than we had before, and the results are starting to look a bit closer to gaussian-ish. 

Next let's take a look at the difficulty and discrimination parameter for each item. 

Note to self: This block of code will need to be fixed if I treat variables as ordinal, as the output of `coef` is now different. 
```{r}
#coefs_2pl <- coef(m1, as.data.frame = TRUE) %>% 
# t() %>%
#  as_tibble() %>%
#  dplyr::select(-c(149:150)) %>%
#  pivot_longer(everything()) %>%
#  separate(, col=name, into=c("item", "parameter"), sep="([.])") %>%
#  pivot_wider(id_cols=item, names_from=parameter, values_from=value) %>%
#  dplyr::select(item, a1, d) %>%
#  mutate(
#   category =  gsub('[[:digit:]]+', '', item)
#  )
  
#ggplot(coefs_2pl,  
#       aes(x = a1, y = d)) + 
#  geom_point(alpha = .3) + 
#  ggrepel::geom_text_repel(data = coefs_2pl, 
#                  aes(label = item), size = 3) + 
#  xlab("Discrimination") + 
#  ylab("Difficulty") + theme_minimal()
```

```{r}
#ggplot(coefs_2pl, aes(x=a1, fill=category)) + geom_histogram() + theme_minimal()
#ggplot(coefs_2pl, aes(x=d, fill=category)) + geom_histogram() + theme_minimal()
```



I was worried that the over-regularization and irregular items would be non-monotonically related to the factor scores, since we expect children to go through qualitatively distinct stages. So I summed these sections and plotted them against the factor score below. 
```{r}
Theta_by_Wordforms <- Complex_short_with_ids %>%
  mutate(
    rn = row_number()
  ) %>%
  transmute(
    Word_form = rowSums(.[,6:30]), 
    Over_Reg = rowSums(.[,31:75]),
    Theta = as.numeric(fscores(m1, "MAP"))
  ) 

p1 <- ggplot(Theta_by_Wordforms, aes(x=Theta, y=Word_form)) + geom_point() + stat_smooth(method="loess")
p2 <- ggplot(Theta_by_Wordforms, aes(x=Theta, y=Over_Reg)) + geom_point() + stat_smooth(method="loess")
p3 <- ggplot(Theta_by_Wordforms, aes(x=Theta)) + geom_histogram()
p4 <- ggplot(Theta_by_Wordforms, aes(x=Word_form)) + geom_histogram()
p5 <- ggplot(Theta_by_Wordforms, aes(x=Over_Reg)) + geom_histogram()
             
(p1 | p2)/(p3 | p4 | p5)

```
Interesting to see that neither is clearly non-monotonic, even though that's what we would expect from theories of language acquisition. 

Now I'm going to try adding each of those sets of variables to the model I fit above. I do the word forms first and then the over-regularization 

# Adding Word Forms. 
```{r}
Complex_short_with_ids <- Complex %>% 
  dplyr::select(data_id, value, out, complexity_category, num_item_id) %>%
  mutate(
    label = str_c(complexity_category, num_item_id)
  ) %>%
  pivot_wider(id_cols=data_id, names_from = "label", values_from="out") %>%
  drop_na()


Complex_short <- Complex_short_with_ids %>%
  dplyr::select(starts_with(c("combine", "morphology", "syntax", "word_forms")), c("word_endings686", "word_endings687", "word_endings688", "word_endings689")) # dataset for IRT can't have IDs
```

```{r}
Complex_poly <- polychoric(Complex_short)

rho <- Complex_poly$rho

lab = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", "14", "15", "16", "17", "18", "19", "20", "21", "22", "23", "24", "25", "26", "27", "28", "29", "30", "31", "32", "33", "34", "35", "36", "37", "38", "39", "40", "41", "42", "43", "44", "45", "46", "47", "48", "49", "50", "51", "52", "53", "54", "55", "56", "57", "58", "59", "60",
"61", "62", "63", "64", "65", "66", "67") # create labels for corPlot because the defintions are long and take up a ton of space
```

```{r}
corPlot(rho, labels=lab)
```

```{r}
pc <- princals(rho)

plot(pc)
```
Looks like there's a big clumb of vectors in one area but then way more pointing out in random directions, which I think will create some fit issues. 

```{r, message=FALSE, error=FALSE}
fa.parallel(rho, fa="fa", cor="poly", n.obs = 2785)
```


```{r, message=FALSE, error=FALSE}
vss(rho, fa="fa", cor="tet", n.obs = 2785)
```

```{r}
m2 <- mirt(Complex_short, 1, itemtype="graded", verbose=FALSE)

summary(m2)

M2(m2)
```


```{r}
m2b <- mirt(Complex_short, 2, "graded", verbose=FALSE)
```

```{r}
summary(m2b, "oblimin", suppress=.20)
```


It looks like the word forms are loading consistently on a different factor from the morphosyntactic complexity factor. 


Another possibility is that all of those items are guesses--especially since they're means are all low. 

# Adding Overregularizations. 
```{r}
Complex_short_with_ids <- Complex %>% 
  dplyr::select(data_id, value, out, complexity_category, num_item_id) %>%
  mutate(
    label = str_c(complexity_category, num_item_id)
  ) %>%
  pivot_wider(id_cols=data_id, names_from = "label", values_from="out") %>%
  drop_na()


Complex_short <- Complex_short_with_ids %>%
  dplyr::select(starts_with(c("combine", "morphology", "syntax", "word_endings"))) # dataset for IRT can't have IDs
```


```{r}
Complex_poly <- polychoric(Complex_short)

rho <- Complex_poly$rho

lab = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", "14", "15", "16", "17", "18", "19", "20", "21", "22", "23", "24", "25", "26", "27", "28", "29", "30", "31", "32", "33", "34", "35", "36", "37", "38", "39", "40", "41", "42", "43", "44", "45", "46", "47", "48", "49", "50", "51", "52", "53", "54", "55", "56", "57", "58", "59", "60",
"61", "62", "63", "64", "65", "66", "67", "68", "69", "70", "71", "72", "73", "74", "75", "76", "77", "78", "79", "80",
"81", "82", "83", "84", "85", "86", "87" ) # create labels for corPlot because the defintions are long and take up a ton of space

corPlot(rho, labels=lab)
pc <- princals(rho)
plot(pc)
```

```{r, message=FALSE, error=FALSE}
fa.parallel(rho, fa="fa", cor="tet", n.obs = 2785)
vss(rho, fa="fa", cor="tet", n.obs = 2785)
```

```{r}
m3b <- mirt(Complex_short, 2, "graded", verbose=FALSE)
summary(m3b, "oblimin", suppress=.20)
```

As was the case with the word form variables, all of the word endings variables loaded onto a separate factor from the morphosyntactic complexity variables.  


# Quick Interim Summary

So a quick re-cap: I was concerned about including the word form and over-regularizations variables to the IRT models because they are theoretically non-monotonically related to syntactic proficiency. We would expect children to initially produce high frequency irregular forms, then go through a stage of over-regularization, and finally to produce the correct irregular word forms again. However, when I plotted the total scores on these subscales against a syntactic proficiency variable derived from the other subscales, I did not see evidence of a non-monotonic relationship. Rather in both cases, subscale scores increased with increasing syntactic proficiency, though it appeared to be leveling off for the over-regularizations.  

I then tried fitting two more models, one that included the word form variables and another that included the overgeneralization variables. Exploration of the dimensionality suggested that adding these variables resulted in a much clearer multi-dimensional structure (see categorical PCAs and scree plots). In both cases, exploratory two dimensional IRTs indicated two-factor solutions in which one factor was dominated by the morphosyntactic complexity variables and the other was dominated by the word form or overregularization variables. 

It's difficult to say exactly what this means, as these results could plausibly be due to different psychological demands of learning the relevant structure or some sort of difference in reporting biases across the scales. 


# Final Model

I think I'm going to stick with the model that includes the complexity variable and the word endings variables as they seem to increase the sensitivity of the model to lower proficiency children and do not appear to affect the dimensionality much. 

Get the data set back. 
```{r}
Complex_short_with_ids <- Complex %>% 
  dplyr::select(data_id, value, out, complexity_category, num_item_id) %>%
  mutate(
    label = str_c(complexity_category, num_item_id)
  ) %>%
  pivot_wider(id_cols=data_id, names_from = "label", values_from="out") %>%
  drop_na()


Complex_short <- Complex_short_with_ids %>%
  dplyr::select(starts_with(c("combine", "morphology", "syntax")), c("word_endings686", "word_endings687", "word_endings688", "word_endings689")) # dataset for IRT can't have IDs
```

```{r}
summary(m1)
```
```{r}
M2(m1)
```
## More formal tests of the dimensionality 
```{r}
m1b <- mirt(Complex_short, 2, "graded", verbose=FALSE)
```

```{r}
anova(m1, m1b)
```

```{r}
summary(m1b, "oblimin", suppress=.2)
```
We have a similar solution to what we had in the first version of this analysis. The factor model fits better but it's not at all clear what the second factor is adding. Everything loads strongly on the first factor and very little on the second factor. 


Comapre with 1 factor graded model with semi-parametric latent variable specification. 
```{r}
m1_nonpar <- mirt(Complex_short, 1, itemtype="graded", dentype = "EH", verbose=FALSE)

M2(m1_nonpar)
```

```{r}
Complex_short %>%
  mutate(
    Theta_EH = fscores(m1_nonpar, method="MAP"), 
    Theta_gauss = fscores(m1, method="MAP")
  ) %>%
  ggplot(aes(x=Theta_EH, y=Theta_gauss)) + geom_point() + stat_smooth(method="loess") + theme_minimal()
```



```{r}
m1_nonpar2 <- mirt(Complex_short, 1, itemtype="graded", dentype = "EHW", verbose=FALSE)

M2(m1_nonpar2)
```

```{r}
Complex_short %>%
  mutate(
    Theta_EH = fscores(m1_nonpar2, method="MAP"), 
    Theta_gauss = fscores(m1, method="MAP")
  ) %>%
  ggplot(aes(x=Theta_EH, y=Theta_gauss)) + geom_point() + stat_smooth(method="loess") + theme_minimal()
```

These semi-parametric alternatives look pretty similar. I think in both cases, the Gaussian model is a bit easier to interpret and will likely make it easier to work with other functionality. 


# Measurement invariance. 

I mentioned in the last meeting that these models assume measusrement invariance -- basically the probability of producing an item is the same for two children of the same estimated ability who differ according to some other grouping variable. I have begun to assess this, using differential item functioning technques using some of the predictors in the data set. I should say, I'm out over my skis a bit here at this point. 



Create data sets with predictor variables for 
```{r}
Complex_short_with_ids <- Complex %>% 
  dplyr::select(data_id, value, out, complexity_category, num_item_id) %>%
  mutate(
    label = str_c(complexity_category, num_item_id)
  ) %>%
  dplyr::select(data_id, label, out) %>%
  pivot_wider(id_cols=data_id, names_from = "label", values_from="out") %>%
  drop_na()
```

```{r}
Complex_short <- Complex_short_with_ids %>%
  dplyr::select(starts_with(c("combine", "morphology", "syntax")), c("word_endings686", "word_endings687", "word_endings688", "word_endings689")) # dataset for IRT can't have IDs
```


```{r}
Complex_with_predictors <- Admin %>%
  dplyr::select(data_id, age, ethnicity, sex) %>%
  mutate(
    female = ifelse(sex=="Female", yes=1, no=0),
    age_y = ifelse(age < 25, yes=1, no=0)
  ) %>%
  right_join(., Complex_short_with_ids, by="data_id")
```

```{r}
Complex_with_predictors %>%
  summarise(
    age_missing = sum(is.na(age)), 
    ethnicity_missing = sum(is.na(ethnicity)), # Some ethnicity variables missing. 
    sex_missing = sum(is.na(sex))
  )

table(Complex_with_predictors$age)
table(Complex_with_predictors$ethnicity)
table(Complex_with_predictors$sex)
```


Use lordif to look for DIF. I'll start with sex first. I've not included the code here, the lordif function, which I used, does not allow verbose=FALSE so it produces a lot of extra information. 
```{r, include=FALSE}
Complex_short2 <- data.frame(Complex_short) 

sex <- data.frame(t = Complex_with_predictors$sex)

dif <- lordif(Complex_short2, 
              Complex_with_predictors$female,
              criterion="Chisqr", 
              alpha = .01)
```

```{r}
dif
```

```{r}
plot(dif)
```

Only two items have DIF and they look pretty similar across the two groups te be honest. 


I've turned off the code
I'll median-split the sample by age, and look for DIF there

```{r, include=FALSE}
Complex_short2 <- data.frame(Complex_short) 

dif <- lordif(Complex_short2, 
              Complex_with_predictors$age_y,
              criterion="Chisqr", 
              alpha = .01)
```

Every single item has Dif:
```{r}
dif
```

I can't get plots I had above, because every item has dif -- the factor scores can't be equated in any reasonable way. 

Plot the distribution of scores by age group to see what's going on. 

```{r}
Complex_with_predictors %>%
  dplyr::select(starts_with(c("morphology", "syntax", "combine")) , c("word_endings686", "word_endings687", "word_endings688", "word_endings689",  "age_y")) %>%
  mutate(
    mean = rowSums(.[1:42])
  ) %>%
  ggplot(aes(y=mean, x=as.factor(age_y))) + geom_boxplot()
```
There are a lot of 0s in the low vocabulary group. I suspect this makes the IRT model difficult to estimate for them, leading to the DIF issues aboe. 


# Plot Theta against age and vocabulary. 
```{r}
by_Participant <-Complex_short_with_ids %>% 
  mutate(
    Theta = fscores(m1, method="MAP")[,1],
    Raw = rowSums(.[2:38])
    ) %>%
  dplyr::select(data_id, Raw, Theta) %>%
  full_join(Complex) %>%
  group_by(data_id) %>%
  slice(1)
```

```{r}
theta_by_age <- by_Participant %>%
  ggplot(aes(x=age, y=Theta)) + geom_point() + stat_smooth(method="loess")

raw_by_age <- by_Participant %>%
  ggplot(aes(x=age, y=Raw)) + geom_point() + stat_smooth(method="loess")

theta_by_age | raw_by_age

```


```{r}
theta_by_prod <- by_Participant %>%
  ggplot(aes(x=production, y=Theta)) + geom_point() + stat_smooth(method="loess")

raw_by_prod <- by_Participant %>%
  ggplot(aes(x=production, y=Raw)) + geom_point() + stat_smooth(method="loess")

theta_by_prod | raw_by_prod

```

```{r}
sessionInfo()
```

















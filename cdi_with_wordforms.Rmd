---
title: Psychometric modeling of the Syntactic Complexity Scale of the CDI
author: Seamus Donnelly
date created: July 5, 2021
date compiled: "`r Sys.Date()`"
output: 
  html_document:
    df_print: paged
    toc: true
    toc_float: true
    number_sections: true
---

```{r message=FALSE, warning=FALSE}
library(wordbankr) # WB data
library(tidyverse) # tidy
library(mirt) # IRT models
library(ltm) # more IRT functions
#library(difr) # some differential item functioning tools. 
library(psych) # some psychometric stuff (tests of dimensionality)
library(Gifi)# some more psychometric stuff (tests of dimensionality)
library(knitr) # some formatting, tables, etc
library(patchwork) # combining plots. 
library(GGally) # More plottinng options. 
library()
```


In our meeting on 28/June/2021, we noted that by adding Word Forms, we might be able to model some of the intermediate levels of syntactic proficiency between the 0 and 1 item by adding word forms. I'm going to start with adding word endings and combines as those seem like the most straight forward. The problem with the word forms and word endings is that we expect them to by non-monotonically related to language proficiency, so I'm no sure how well they'll fit this model. 


Download data from Word Bank:
```{r get data}
Inst <- get_instrument_data(language="English (American)", form="WS")
Admin <- get_administration_data(language="English (American)", form="WS")
Item <- get_item_data(language="English (American)", form = "WS")
```


Combine data sets. Create binary variable for accuracy.  
```{r format data}
Complex <- Admin %>%
  full_join(.,Inst, by="data_id") %>%
  full_join(., Item, by="num_item_id") %>%
  filter(longitudinal==FALSE) %>%
  filter(type == "combine" | 
           type == "complexity" |
           type == "word_endings" |
           type== "word_forms_nouns" |
           type == "word_forms_verbs" | 
           type == "word_endings_nouns" |
           type == "word_endings_verbs"
         ) %>%
  mutate(
    out = ifelse(value=="complex" | value=="sometimes" | value=="often" | value=="produces", yes=1, no=0)
  ) 

```


## Data Screening
Let's see how many missing values there are. 
```{r count NAs}
Complex %>%
  filter(is.na(out)) %>%
  group_by(definition) %>%
  count() 
```

Looks like there are 1426 participants don't have item-level complexity scores. There are a further 23 participants who don't have word ending scores, but do have syntactic complexity. I think that IRT models can handle this missing data. 
```{r Drop NAs}
Complex$complexity_category <- ifelse(Complex$complexity_category == "", yes=Complex$type, no=Complex$complexity_category)
```

Let's look at means and SDs for each item:
```{r Summary Tables}
Complex %>% 
  filter(!is.na(out)) %>%
  group_by(definition) %>%
  summarise(
    mean=mean(out), 
    sd=sd(out),
    category = first(complexity_category)
  ) %>%
  arrange(category) %>%
  kable(caption="Means and SDs for Each Item (Arranged by Item)")
```


Let's also check the distribution of means for each category
```{r}
Complex %>%
  filter(!is.na(out)) %>%
  group_by(definition) %>%
  summarise(
    mean=mean(out), 
    category = first(complexity_category)
  ) %>%
  ggplot(aes(x=category, y= mean)) + geom_boxplot()
```



## Data Preparation
Prepare data set for IRT modeling. I'm going to re-name each item to its category (morphology or syntax) and its item number so as to make some graphs easier to read. 
```{r}
Complex_short_with_ids <- Complex %>% 
  dplyr::select(data_id, value, out, complexity_category, num_item_id) %>%
  mutate(
    label = str_c(complexity_category, num_item_id)
  ) %>%
  pivot_wider(id_cols=data_id, names_from = "label", values_from="out") %>%
  drop_na()


Complex_short <- Complex_short_with_ids %>%
  dplyr::select(starts_with(c("combine", "morphology", "syntax")), c("word_endings686", "word_endings687", "word_endings688", "word_endings689")) # dataset for IRT can't have IDs
```

## Dimensionality Assessment
Prior to fitting the IRT model, it's worth looking at the correlation structure of the data to get a sense of the possible dimensionality. 

Create tetrachoric correlation matrix (for binary data). I created a vector with labels, for labeling of the correlation plot. 
```{r}
Complex_poly <- tetrachoric(Complex_short, correct=.001)

rho <- Complex_poly$rho

lab = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", "14", "15", "16", "17", "18", "19", "20", "21", "22", "23", "24", "25", "26", "27", "28", "29", "30", "31", "32", "33", "34", "35", "36", "37", "38", "39", "40", "41", "42") # create labels for corPlot because the defintions are long and take up a ton of space
```
Check out the correlations
```{r}
corPlot(rho, labels=lab)
```

They look really high. Let's examine the dimensionality, using princals, which is basically just a version of PCA that is appropriate for categorical data. 
```{r}
pc <- princals(rho)

plot(pc)
```

Interesting. Seems like maybe there are 2 factors. An alternative approach is a scree plot with confidence intervals from simulated data:
```{r}
fa.parallel(rho, fa="fa", cor="tet", correct=.001, n.obs = 2785)
```

Parallel analysis suggests one very large factor and a small second one. A third metric is very simple structure. This tells us the best solution assuming different values of k, where k is the maximum number of latent variables an observed variable loads on. 
```{r}
vss(rho, cor="tet", n.obs = 2785)
```

Very simple structure also suggests one, if we assume each variable loads on one factor only. If we assume everything loads on 2 factors then a 2 factor solution is best 


These tests seem to suggest a single factor is probably the best approach, though it's possible a two factor approach would be better. 

# IRT Modeling

## 2PL model
Let's first fit the 2pl model and examine the output. 
```{r  message=FALSE, warning=FALSE}
m1 <- mirt(Complex_short, 1, itemtype="2PL", verbose=FALSE)
```

```{r}
m1
```

```{r}
summary(m1)
```


Let's check the overall fit of the model. The chi square statistic here isn't super meaningful given the sample size.  
```{r}
M2(m1)
```

RMSEA and CFI are good, SRMSR is a bit high but pretty reasonable. We can see if individual items misfit -- i.e., do the observed cell counts differ from the expected cell counts. 
```{r}
itemfit(m1)
```

A few items significantly misfit. Let's take everything with a p value less than .10. We'll look at item gam plots of those, which plot the item response probability against the latent variable using a gam function, to look for deviations in the item-response function.   

```{r, message=FALSE, warning=FALSE}
misfit <- itemfit(m1) %>% # Get labels of mis-fitting items. 
  filter(p.S_X2 <= .10) %>%
  dplyr::select(item) %>%
  as.vector()

items_good <- dplyr::select(Complex_short, -all_of(misfit$item)) # Well fitting items
items_bad <- dplyr::select(Complex_short, all_of(misfit$item)) # Poorly fitting items

mod_fit <- mirt(items_good, 1, "2PL", verbose=FALSE) # Calculate factor scores using only the well fitting items. 
Theta <- fscores(mod_fit)


```
# Plot non-linear item response theory curves with the factor scores 

```{r}
IG760 <- itemGAM(items_bad$combine760, Theta)

IG761 <- itemGAM(items_bad$morphology761,Theta)
IG762 <- itemGAM(items_bad$morphology762,Theta)
IG763 <- itemGAM(items_bad$morphology763,Theta)
IG769<- itemGAM(items_bad$morphology769,Theta)


IG775 <- itemGAM(items_bad$syntax775,Theta)
IG776 <- itemGAM(items_bad$syntax776,Theta)
IG789 <- itemGAM(items_bad$syntax789,Theta)
IG792 <- itemGAM(items_bad$syntax792,Theta)
```


Here are the morphology items
```{r}
plot(IG760)
```

```{r}
plot(IG761); plot(IG762); plot(IG763); plot(IG769)

```
Syntax items
```{r}
plot(IG775); plot(IG776); plot(IG789); plot(IG792)
```
I'm not sure what's happening with item 775. Why is the probability of a correct response lower at the highest values? This wasn't the case before the other variables were added. 


These don't look like massive deviations. They are all monotonic, certainly, but they seem to slow down more than the logistic function. 
 
Next let's look at the relationship between the true scores and the raw scores. 
```{r}
true_raw <- Complex_short %>%
  mutate(
    Raw = rowSums(.[,1:42]), 
    Theta = fscores(m1, method="MAP")
  ) %>%
  ggplot(aes(x=Theta, y=Raw)) + geom_point() + stat_smooth(method="loess") + theme_minimal()

raw_score <- Complex_short %>%
  mutate(
    Raw = rowSums(.[,1:42])
    ) %>% 
   ggplot(aes(x=Raw)) + geom_histogram() + theme_minimal()

true_score <- Complex_short %>%
  mutate(
    Theta = fscores(m1, method="MAP")
    ) %>% 
   ggplot(aes(x=Theta)) + geom_histogram() + theme_minimal()

library(patchwork)

true_raw/(true_score + raw_score) 
```

Two things suggest this is a more precise measurement: the relationship between the true and raw scores is sigmoid-ish, and there are multiple values of the true score at every level of the observed score. Also, note the number of 0s on the raw score, and really high scores on the true score. 


Next let's take a look at the difficulty and discrimination parameter for each item. 
```{r}
coefs_2pl <- coef(m1, as.data.frame = TRUE) %>% 
 t() %>%
  as_tibble() %>%
  dplyr::select(-c(149:150)) %>%
  pivot_longer(everything()) %>%
  separate(, col=name, into=c("item", "parameter"), sep="([.])") %>%
  pivot_wider(id_cols=item, names_from=parameter, values_from=value) %>%
  dplyr::select(item, a1, d) %>%
  mutate(
   category =  gsub('[[:digit:]]+', '', item)
  )
  
```

```{r}
ggplot(coefs_2pl,  
       aes(x = a1, y = d)) + 
  geom_point(alpha = .3) + 
  ggrepel::geom_text_repel(data = coefs_2pl, 
                  aes(label = item), size = 3) + 
  xlab("Discrimination") + 
  ylab("Difficulty") + theme_minimal()
```

```{r}
ggplot(coefs_2pl, aes(x=a1, fill=category)) + geom_histogram() + theme_minimal()
ggplot(coefs_2pl, aes(x=d, fill=category)) + geom_histogram() + theme_minimal()
```

```{r}
Complex_short_with_ids %>%
  transmute(
    Word_form = rowSums(.[,6:30]), 
    OverReg = rowSums(.[,31:75]),
    Theta = fscores(m1, "MAP")
  ) %>%
  ggpairs()
```
OK, something is wrong here. 












Strong correlation between the difficulty and discrimination parameter. Also note that the morphology items are less difficult than the syntax ones in general. 




## Thinking Through the Assumptions of the 2PL model 
It's important to remember that these latent variables are only interpretable if the following conditions hold:
1. The assumption of unidimensionality is appropriate. 
2. The assumed distributional form of the latent variable is correct. 
3. Measurement invariance holds: the items behave the same way across different groups of participants.

I can test 1 and 2 on my own. 3 is a bit of a stretch for me right now. 


### Non-parametric model relaxing assumption of normal latent variable. 

Let's look at the distribution of the latent variable first. MIRT allows us to relax the assumption of normally distributed latent variables at the cost of loss of flexibility. I'll look at a model which makes no assumptions about the form of the latent variable **note to self: check this later** , and compare results to those above. 

```{r, warning=TRUE, message=TRUE}
m1_nonpar <- mirt(Complex_short, 1, itemtype="2PL", dentype = "EH", verbose=FALSE)

M2(m1_nonpar)
```

RMSREA is a bit lower than the model above. The standarized residuals are quite large though. 

```{r}
Complex_short %>%
  mutate(
    Theta_EH = fscores(m1_nonpar, method="MAP"), 
    Theta_gauss = fscores(m1, method="MAP")
  ) %>%
  ggplot(aes(x=Theta_EH, y=Theta_gauss)) + geom_point() + stat_smooth(method="loess") + theme_minimal()

```

These are quite similar though a bit different in the tails. There's another type of non-parametric latent variable in MIRT. 

```{r}
m1_nonpar2 <- mirt(Complex_short, 1, itemtype="2PL", dentype = "EHW", verbose=FALSE)

M2(m1_nonpar2)

Complex_short %>%
  mutate(
    Theta_EH2 = fscores(m1_nonpar2, method="MAP"), 
    Theta_gauss = fscores(m1, method="MAP")
  ) %>%
  ggplot(aes(x=Theta_EH2, y=Theta_gauss)) + geom_point() + stat_smooth(method="loess") + theme_minimal()
```

These are also very, very close. I think it's sensible to stick with the Gaussian model. It fits better, its simpler and more flexible, and the factor scores are really, really similar across the two approaches. 


The next thing we can do is check the dimensionality. I got mixed results before. 
Check dimensionality against 2 dimensional model. 

### Test assumption of unidimensionality 

First a formal test of dimensionality. We need to fit the model with ltm. The unidimTest takes forever to run and produces enormous output, so it's commented out. 
```{r}
#m1_ltm <- ltm(Complex_short ~z1)
#unidimTest(m1_ltm, Complex_short)
```

Looks like this test of uni-dimensionality has been rejected. Let's try explicitly modeling the multidimensionality. 


Exploratory 2 dimensional IRT. 
```{r, warning=TRUE, message=TRUE}
m2 <- mirt(Complex_short, 2, itemtype="2PL", verbose=FALSE)
```


```{r}
M2(m2)
anova(m1, m2)
summary(m2, "oblimin", suppress=.20)
```

This model fits better, but it doesn't look like the second factor is adding much of anything. Also it looks like the correlation between factors is really small, which seems unlikely given the way language works. 


Plot the discrimination parameters for each dimension against one another. 
```{r}
coefs_2plmulti <- coef(m2, as.data.frame = TRUE) %>% 
 t() %>%
  as_tibble() %>%
  dplyr::select(-c(185:190)) %>%
  pivot_longer(everything()) %>%
  separate(, col=name, into=c("item", "parameter"), sep="([.])") %>%
  pivot_wider(id_cols=item, names_from=parameter, values_from=value) %>%
  dplyr::select(item, a1, a2, d) %>%
  mutate(
   category =  gsub('[[:digit:]]+', '', item)
  ) 
ggplot(coefs_2plmulti ,  
       aes(x = a1, y = a2)) + 
  geom_point(alpha = .3) + 
  ggrepel::geom_text_repel(data =coefs_2plmulti, 
                  aes(label = item), size = 3) + 
  xlab("Discrimination1") + 
  ylab("Discrimination2") + theme_minimal()
```
2 discrimination parameters. They're both negative. I'm wondering if these are parameterized a little bit differently, such that they mean something different in the two dimensional context. 

Plot the latent variables against the raw scores and against one another. 
```{r}
Complex_short %>%
  mutate(
    Raw = rowSums(.[,1:42]), 
    Theta1 = fscores(m2, method="MAP")[,1], 
    Theta2 = fscores(m2, method="MAP")[,2]
  ) %>%
  dplyr::select(Raw, Theta1, Theta2) %>%
  ggpairs()

```

The correlation between the raw scores and Theta1 is extremely high, whereas the correlation between theta2 and the raw score is minimal. It also has a much smaller range. There's also a weak correlation between the factor scores. 


# Adding the word form variables. 

```{r}

```


```{r}

```


```{r}

```


```{r}

```


```{r}

```


```{r}

```


